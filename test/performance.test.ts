import { describe, it, expect, beforeEach, afterEach } from 'vitest';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { execFile } from 'child_process';
import { promisify } from 'util';
import { scanPath } from '../src/scanner.js';

const execFileAsync = promisify(execFile);

describe('performance benchmarks', () => {
  let tmpDir: string;
  let gitRepo: string;

  beforeEach(async () => {
    tmpDir = await fs.mkdtemp(path.join(os.tmpdir(), 'sentinel-perf-'));
    gitRepo = path.join(tmpDir, 'repo');
    await fs.mkdir(gitRepo);

    // Initialize git repo
    await execFileAsync('git', ['init'], { cwd: gitRepo });
    await execFileAsync('git', ['config', 'user.email', 'bench@example.com'], { cwd: gitRepo });
    await execFileAsync('git', ['config', 'user.name', 'Bench User'], { cwd: gitRepo });
  });

  afterEach(async () => {
    try {
      await fs.rm(tmpDir, { recursive: true, force: true });
    } catch {}
  });

  it('benchmark: full scan vs cached scan performance', async () => {
    // Create test dataset: 100 files with 10 having secrets
    for (let i = 0; i < 100; i++) {
      const hasSecret = i % 10 === 0;
      const content = hasSecret
        ? `File ${i}\naws_access_key_id = AKIA${i.toString().padStart(16, '0')}\nMore content`
        : `File ${i}\nClean content\nNo secrets here`;
      await fs.writeFile(path.join(gitRepo, `file${i}.txt`), content);
    }

    // Commit files
    await execFileAsync('git', ['add', '.'], { cwd: gitRepo });
    await execFileAsync('git', ['commit', '-m', 'initial'], { cwd: gitRepo });

    const cachePath = path.join(tmpDir, 'cache.json');

    // Benchmark 1: Full scan without cache
    const start1 = Date.now();
    const findings1 = await scanPath(gitRepo, [], gitRepo, { concurrency: 8 });
    const duration1 = Date.now() - start1;

    expect(findings1.length).toBeGreaterThan(0);
    console.log(`[PERF] Full scan (no cache): ${duration1}ms, ${findings1.length} findings`);

    // Benchmark 2: Full scan with cache (first run to populate)
    const start2 = Date.now();
    const findings2 = await scanPath(gitRepo, [], gitRepo, {
      concurrency: 8,
      cachePath,
    });
    const duration2 = Date.now() - start2;

    expect(findings2.length).toBe(findings1.length);
    console.log(`[PERF] Full scan (populate cache): ${duration2}ms, ${findings2.length} findings`);

    // Benchmark 3: Full scan with cache (cache hit)
    const start3 = Date.now();
    const findings3 = await scanPath(gitRepo, [], gitRepo, {
      concurrency: 8,
      cachePath,
    });
    const duration3 = Date.now() - start3;

    expect(findings3.length).toBe(findings1.length);
    console.log(`[PERF] Full scan (cache hit): ${duration3}ms, ${findings3.length} findings`);

    // Cache should improve performance (may vary on CI)
    // Just log speedup for performance tracking
    const speedup = duration3 > 0 ? duration1 / duration3 : 1;
    console.log(`[PERF] Cache speedup: ${speedup.toFixed(2)}x`);

    // Verify findings are consistent
    expect(findings3.length).toBe(findings1.length);
  }, 10000); // 10 second timeout for large file set

  it('benchmark: incremental scan vs full scan', async () => {
    // Create initial dataset
    for (let i = 0; i < 50; i++) {
      const content = `File ${i}\nInitial content\nNo secrets`;
      await fs.writeFile(path.join(gitRepo, `file${i}.txt`), content);
    }

    // Commit all files
    await execFileAsync('git', ['add', '.'], { cwd: gitRepo });
    await execFileAsync('git', ['commit', '-m', 'initial'], { cwd: gitRepo });

    const cachePath = path.join(tmpDir, 'cache.json');

    // Populate cache
    await scanPath(gitRepo, [], gitRepo, { cachePath });

    // Modify only 5 files (10%)
    for (let i = 0; i < 5; i++) {
      const content = `File ${i}\nModified content\naws_access_key_id = AKIA${i.toString().padStart(16, '0')}`;
      await fs.writeFile(path.join(gitRepo, `file${i}.txt`), content);
    }

    // Benchmark 1: Full scan (cache)
    const start1 = Date.now();
    const findings1 = await scanPath(gitRepo, [], gitRepo, {
      cachePath,
      incremental: false,
    });
    const duration1 = Date.now() - start1;

    console.log(`[PERF] Full scan (with cache): ${duration1}ms, ${findings1.length} findings`);

    // Re-populate cache after full scan
    await scanPath(gitRepo, [], gitRepo, { cachePath });

    // Modify same files again
    for (let i = 0; i < 5; i++) {
      const content = `File ${i}\nModified again\naws_access_key_id = AKIA${i.toString().padStart(16, '0')}`;
      await fs.writeFile(path.join(gitRepo, `file${i}.txt`), content);
    }

    // Benchmark 2: Incremental scan
    const start2 = Date.now();
    const findings2 = await scanPath(gitRepo, [], gitRepo, {
      cachePath,
      incremental: true,
    });
    const duration2 = Date.now() - start2;

    console.log(`[PERF] Incremental scan: ${duration2}ms, ${findings2.length} findings`);

    // Incremental may have git overhead on small datasets, so just log the comparison
    // In real-world large repos, incremental will be much faster
    const speedup = duration2 > 0 ? duration1 / duration2 : 1;
    console.log(`[PERF] Incremental speedup: ${speedup.toFixed(2)}x`);

    // Just verify it completes successfully
    expect(findings2.length).toBeGreaterThan(0);
  }, 10000); // 10 second timeout

  it('benchmark: concurrent scanning scaling', async () => {
    // Create moderate dataset
    for (let i = 0; i < 50; i++) {
      const hasSecret = i % 5 === 0;
      const content = hasSecret
        ? `aws_access_key_id = AKIA${i.toString().padStart(16, '0')}`
        : `Clean file ${i}`;
      await fs.writeFile(path.join(gitRepo, `file${i}.txt`), content);
    }

    const concurrencies = [1, 2, 4, 8];
    const results: Array<{ concurrency: number; duration: number }> = [];

    for (const conc of concurrencies) {
      const start = Date.now();
      const findings = await scanPath(gitRepo, [], gitRepo, { concurrency: conc });
      const duration = Date.now() - start;

      results.push({ concurrency: conc, duration });
      console.log(`[PERF] Concurrency ${conc}: ${duration}ms, ${findings.length} findings`);
      expect(findings.length).toBeGreaterThan(0);
    }

    // Log concurrency scaling (performance can vary by platform)
    const speedup = results[0].duration / results[3].duration;
    console.log(`[PERF] Concurrency scaling: 1â†’8 speedup = ${speedup.toFixed(2)}x`);

    // Verify reasonable speedup (at least some benefit, but platform-dependent)
    expect(speedup).toBeGreaterThan(0.8); // Allow for platform differences
  });

  it('benchmark: cache modes (mtime vs hash)', async () => {
    // Create test files
    for (let i = 0; i < 30; i++) {
      const content = `File ${i}\naws_access_key_id = AKIA${i.toString().padStart(16, '0')}`;
      await fs.writeFile(path.join(gitRepo, `file${i}.txt`), content);
    }

    const cacheMtime = path.join(tmpDir, 'cache-mtime.json');
    const cacheHash = path.join(tmpDir, 'cache-hash.json');

    // Benchmark mtime mode
    const env1 = { ...process.env, SENTINEL_CACHE_MODE: 'mtime' };
    const start1 = Date.now();
    await scanPath(gitRepo, [], gitRepo, { cachePath: cacheMtime });
    const duration1 = Date.now() - start1;

    const start1b = Date.now();
    await scanPath(gitRepo, [], gitRepo, { cachePath: cacheMtime });
    const duration1b = Date.now() - start1b;

    console.log(`[PERF] mtime mode (populate): ${duration1}ms`);
    console.log(`[PERF] mtime mode (cached): ${duration1b}ms`);

    // Benchmark hash mode
    const env2 = { ...process.env, SENTINEL_CACHE_MODE: 'hash' };
    const start2 = Date.now();
    await scanPath(gitRepo, [], gitRepo, { cachePath: cacheHash });
    const duration2 = Date.now() - start2;

    const start2b = Date.now();
    await scanPath(gitRepo, [], gitRepo, { cachePath: cacheHash });
    const duration2b = Date.now() - start2b;

    console.log(`[PERF] hash mode (populate): ${duration2}ms`);
    console.log(`[PERF] hash mode (cached): ${duration2b}ms`);

    // Log cache mode comparison (exact timing varies by platform)
    const populateRatio = duration2 / duration1;
    const cachedRatio = duration2b / duration1b;
    console.log(`[PERF] mtime vs hash (populate): ${populateRatio.toFixed(2)}x`);
    console.log(`[PERF] mtime vs hash (cached): ${cachedRatio.toFixed(2)}x`);

    // Verify hash mode is slower but reasonable (platform-dependent)
    expect(duration2).toBeGreaterThan(0); // Just ensure it completed
  });

  it('benchmark: large file handling', async () => {
    // Create one large file with many lines
    const lines: string[] = [];
    for (let i = 0; i < 10000; i++) {
      if (i % 1000 === 0) {
        lines.push(`Line ${i}: aws_access_key_id = AKIA${i.toString().padStart(16, '0')}`);
      } else {
        lines.push(`Line ${i}: Normal content goes here`);
      }
    }
    await fs.writeFile(path.join(gitRepo, 'large.txt'), lines.join('\n'));

    const start = Date.now();
    const findings = await scanPath(gitRepo, [], gitRepo, { concurrency: 1 });
    const duration = Date.now() - start;

    console.log(
      `[PERF] Large file (10k lines): ${duration}ms, ${findings.length} findings, ${(10000 / (duration / 1000)).toFixed(0)} lines/sec`,
    );

    expect(findings.length).toBeGreaterThan(0);
    expect(duration).toBeLessThan(5000); // Should process in under 5 seconds
  });
});
